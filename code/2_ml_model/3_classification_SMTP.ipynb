{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os  # For interacting with the file system\n",
    "import pandas as pd  # For handling dataframes and CSVs\n",
    "import numpy as np\n",
    "from elasticsearch import helpers, Elasticsearch\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm  # Import tqdm for progress tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'MSI', 'cluster_name': 'elasticsearch', 'cluster_uuid': 'ylmZI0lnRpa-pP1lwEKJ7A', 'version': {'number': '8.15.2', 'build_flavor': 'default', 'build_type': 'zip', 'build_hash': '98adf7bf6bb69b66ab95b761c9e5aadb0bb059a3', 'build_date': '2024-09-19T10:06:03.564235954Z', 'build_snapshot': False, 'lucene_version': '9.11.1', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n",
      "Connected to Elasticsearch\n"
     ]
    }
   ],
   "source": [
    "# Retrieve authentication information for Elasticsearch\n",
    "elastic_host = \"https://localhost\"\n",
    "elastic_port = \"9200\"\n",
    "elastic_user = \"admin\"\n",
    "elastic_password = \"motdepasse\"\n",
    "elastic_ca_path = \"C:\\\\elasticsearch-8.15.2\\\\config\\\\certs\\\\http_ca.crt\"\n",
    "\n",
    "# Connect to Elasticsearch\n",
    "es = Elasticsearch(\n",
    "    hosts=[f\"{elastic_host}:{elastic_port}\"],\n",
    "    basic_auth=(elastic_user, elastic_password),\n",
    "    ca_certs=elastic_ca_path,\n",
    "    verify_certs=True\n",
    ")\n",
    "print(es.info())\n",
    "\n",
    "# Check connection\n",
    "if es.ping():\n",
    "    print(\"Connected to Elasticsearch\")\n",
    "else:\n",
    "    print(\"Failed to connect to Elasticsearch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_flows_from_elasticsearch(application_name):\n",
    "    data = []\n",
    "    \n",
    "    # Define the body with a filter on application_name\n",
    "    body = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"application_name\": application_name\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    res = helpers.scan(\n",
    "                    client=es,\n",
    "                    scroll='2m',\n",
    "                    query=body,\n",
    "                    index=\"network_flows_fan_encoded_final\")\n",
    "    \n",
    "    for i in res:\n",
    "        data.append(i['_source'])\n",
    "    \n",
    "    # Converting into a Pandas dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Print the dataframe\n",
    "    print(f\"Network data : \\n{df}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network data : \n",
      "      application_name  bidirectional_packets  bidirectional_bytes  \\\n",
      "0                 SMTP               0.000757            -0.042466   \n",
      "1                 SMTP               0.011104            -0.042444   \n",
      "2                 SMTP               0.011104            -0.041705   \n",
      "3                 SMTP               0.011104            -0.042444   \n",
      "4                 SMTP              -0.019937            -0.044728   \n",
      "...                ...                    ...                  ...   \n",
      "43808             SMTP               0.011104            -0.041223   \n",
      "43809             SMTP               0.011104            -0.041772   \n",
      "43810             SMTP               0.011104            -0.041223   \n",
      "43811             SMTP               0.011104            -0.041772   \n",
      "43812             SMTP               0.000757            -0.042533   \n",
      "\n",
      "       bidirectional_mean_ps  bidirectional_stddev_ps  src2dst_mean_ps  \\\n",
      "0                   0.923917                 1.233186         3.653319   \n",
      "1                   0.875340                 1.209235         3.637574   \n",
      "2                   0.882382                 1.216835         3.653319   \n",
      "3                   0.875340                 1.209235         3.637574   \n",
      "4                   1.005130                 1.258629         3.637574   \n",
      "...                      ...                      ...              ...   \n",
      "43808               0.886970                 1.222084         3.663576   \n",
      "43809               0.881741                 1.216122         3.651887   \n",
      "43810               0.886970                 1.222084         3.663576   \n",
      "43811               0.881741                 1.216122         3.651887   \n",
      "43812               0.923263                 1.232468         3.651887   \n",
      "\n",
      "       src2dst_stddev_ps  dst2src_mean_ps  dst2src_stddev_ps  \\\n",
      "0               3.528862        -0.781282          -0.795256   \n",
      "1               3.517386        -0.785470          -0.799802   \n",
      "2               3.528862        -0.785470          -0.799802   \n",
      "3               3.517386        -0.785470          -0.799802   \n",
      "4               3.517386        -0.771509          -0.785368   \n",
      "...                  ...              ...                ...   \n",
      "43808           3.537010        -0.785470          -0.799802   \n",
      "43809           3.527767        -0.785470          -0.799802   \n",
      "43810           3.537010        -0.785470          -0.799802   \n",
      "43811           3.527767        -0.785470          -0.799802   \n",
      "43812           3.527767        -0.781282          -0.795256   \n",
      "\n",
      "       bidirectional_mean_piat_ms  ...  protocol_6  protocol_17  protocol_89  \\\n",
      "0                       -0.042363  ...        True        False        False   \n",
      "1                       -0.050617  ...        True        False        False   \n",
      "2                       -0.050185  ...        True        False        False   \n",
      "3                       -0.050976  ...        True        False        False   \n",
      "4                       -0.033055  ...        True        False        False   \n",
      "...                           ...  ...         ...          ...          ...   \n",
      "43808                   -0.050509  ...        True        False        False   \n",
      "43809                   -0.014200  ...        True        False        False   \n",
      "43810                   -0.035878  ...        True        False        False   \n",
      "43811                   -0.021965  ...        True        False        False   \n",
      "43812                   -0.029835  ...        True        False        False   \n",
      "\n",
      "       protocol_132  src_port_class_Dynamic  src_port_class_Registered  \\\n",
      "0             False                   False                       True   \n",
      "1             False                    True                      False   \n",
      "2             False                   False                       True   \n",
      "3             False                   False                       True   \n",
      "4             False                   False                       True   \n",
      "...             ...                     ...                        ...   \n",
      "43808         False                   False                       True   \n",
      "43809         False                    True                      False   \n",
      "43810         False                    True                      False   \n",
      "43811         False                    True                      False   \n",
      "43812         False                   False                       True   \n",
      "\n",
      "       src_port_class_WellKnown  dst_port_class_Dynamic  \\\n",
      "0                         False                   False   \n",
      "1                         False                   False   \n",
      "2                         False                   False   \n",
      "3                         False                   False   \n",
      "4                         False                   False   \n",
      "...                         ...                     ...   \n",
      "43808                     False                   False   \n",
      "43809                     False                   False   \n",
      "43810                     False                   False   \n",
      "43811                     False                   False   \n",
      "43812                     False                   False   \n",
      "\n",
      "       dst_port_class_Registered  dst_port_class_WellKnown  \n",
      "0                          False                      True  \n",
      "1                          False                      True  \n",
      "2                          False                      True  \n",
      "3                          False                      True  \n",
      "4                          False                      True  \n",
      "...                          ...                       ...  \n",
      "43808                      False                      True  \n",
      "43809                      False                      True  \n",
      "43810                      False                      True  \n",
      "43811                      False                      True  \n",
      "43812                      False                      True  \n",
      "\n",
      "[43813 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "df_http = fetch_flows_from_elasticsearch(\"SMTP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "# Assuming 'application_name' is the target column in df_http for stratification\n",
    "# Separate features and target\n",
    "X = df_http.drop(columns=[\"label\", \"application_name\"])  # Features\n",
    "y = df_http[\"label\"]  # Target variable\n",
    "\n",
    "# Step 1: Split data into 80% train and 20% test, with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    43067\n",
       "1      746\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_http[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    8614\n",
       "1     149\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Partition X_train and y_train into 5 stratified subsets\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_subsets = []\n",
    "\n",
    "for train_index, val_index in skf.split(X_train, y_train):\n",
    "    # Create subsets S1, S2, S3, S4, and S5 as (X, y) pairs\n",
    "    subset_X, subset_y = X_train.iloc[train_index], y_train.iloc[train_index]\n",
    "    train_subsets.append((subset_X, subset_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Size: 8763\n",
      "Subset S1 Size: 28040\n",
      "Subset S2 Size: 28040\n",
      "Subset S3 Size: 28040\n",
      "Subset S4 Size: 28040\n",
      "Subset S5 Size: 28040\n"
     ]
    }
   ],
   "source": [
    "# Optional: Print summary of the subsets\n",
    "print(f\"Test Set Size: {len(X_test)}\")\n",
    "for i, (subset_X, subset_y) in enumerate(train_subsets, start=1):\n",
    "    print(f\"Subset S{i} Size: {len(subset_X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize an empty list to hold evaluation metrics for each k\n",
    "results = []\n",
    "\n",
    "# Créer une figure pour les courbes ROC\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Couleurs différentes pour chaque k\n",
    "colors = ['blue', 'red', 'green', 'purple']\n",
    "\n",
    "for k_idx, k in enumerate(range(1,5)):\n",
    "    # Initialize an empty list to hold evaluation metrics for each T_1\n",
    "    local_results = []\n",
    "    \n",
    "    # Listes pour stocker les taux moyens de faux positifs et vrais positifs\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    \n",
    "    # Run 5 tasks, each with a different S_i for testing and T_i for training\n",
    "    for i in range(5):\n",
    "        # S_i is the i-th subset used for testing\n",
    "        X_test_task = train_subsets[i][0]\n",
    "        y_test_task = train_subsets[i][1]\n",
    "        \n",
    "        # T_i is the union of all subsets except S_i\n",
    "        X_train_task = pd.concat([train_subsets[j][0] for j in range(5) if j != i])\n",
    "        y_train_task = pd.concat([train_subsets[j][1] for j in range(5) if j != i])\n",
    "        \n",
    "        # Initialize the KNN classifier\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        \n",
    "        # Fit the model on T_i\n",
    "        knn.fit(X_train_task, y_train_task)\n",
    "        \n",
    "        # Predict probabilities and classes\n",
    "        y_pred_proba = knn.predict_proba(X_test_task)[:, 1]\n",
    "        y_pred = knn.predict(X_test_task)\n",
    "        \n",
    "        # Calculer la courbe ROC\n",
    "        fpr, tpr, _ = roc_curve(y_test_task, y_pred_proba)\n",
    "        \n",
    "        # Interpoler pour avoir des points uniformes\n",
    "        interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        \n",
    "        # Calculer l'AUC pour ce fold\n",
    "        auc = roc_auc_score(y_test_task, y_pred_proba)\n",
    "        aucs.append(auc)\n",
    "        \n",
    "        # Calculate other evaluation metrics\n",
    "        accuracy = accuracy_score(y_test_task, y_pred)\n",
    "        precision = precision_score(y_test_task, y_pred, pos_label=1)\n",
    "        recall = recall_score(y_test_task, y_pred, pos_label=1)\n",
    "        f1 = f1_score(y_test_task, y_pred, pos_label=1)\n",
    "        \n",
    "        # Store the local results for this Task i\n",
    "        local_results.append({\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1,\n",
    "            \"AUC\": auc\n",
    "        })\n",
    "    \n",
    "    # Calculer la courbe ROC moyenne\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = np.mean(aucs)\n",
    "    std_auc = np.std(aucs)\n",
    "    \n",
    "    # Tracer la courbe ROC moyenne\n",
    "    plt.plot(mean_fpr, mean_tpr, color=colors[k_idx],\n",
    "             label=f'ROC k={k} (AUC = {mean_auc:.2f} ± {std_auc:.2f})',\n",
    "             lw=2, alpha=0.8)\n",
    "    \n",
    "    # Calculate averages for other metrics\n",
    "    avg_accuracy = sum(d[\"Accuracy\"] for d in local_results) / 5\n",
    "    avg_precision = sum(d[\"Precision\"] for d in local_results) / 5\n",
    "    avg_recall = sum(d[\"Recall\"] for d in local_results) / 5\n",
    "    avg_f1 = sum(d[\"F1 Score\"] for d in local_results) / 5\n",
    "    \n",
    "    # Append results\n",
    "    results.append({\n",
    "        \"k\": k,\n",
    "        \"Average AUC\": mean_auc,\n",
    "        \"Average Accuracy\": avg_accuracy,\n",
    "        \"Average Precision\": avg_precision,\n",
    "        \"Average Recall\": avg_recall,\n",
    "        \"Average F1 Score\": avg_f1\n",
    "    })\n",
    "\n",
    "# Finaliser le graphique ROC\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', alpha=0.8,\n",
    "         label='Chance (AUC = 0.5)')\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title('Courbes ROC moyennes pour différentes valeurs de k')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Afficher les résultats pour chaque k\n",
    "for result in results:\n",
    "    print(f\"\\nRésultats pour k={result['k']}:\")\n",
    "    print(f\"AUC moyen: {result['Average AUC']:.4f}\")\n",
    "    print(f\"Accuracy moyen: {result['Average Accuracy']:.4f}\")\n",
    "    print(f\"Precision moyen: {result['Average Precision']:.4f}\")\n",
    "    print(f\"Recall moyen: {result['Average Recall']:.4f}\")\n",
    "    print(f\"F1-score moyen: {result['Average F1 Score']:.4f}\")\n",
    "\n",
    "# Trouver le meilleur k basé sur l'AUC\n",
    "best_k = max(results, key=lambda x: x['Average AUC'])\n",
    "print(f\"\\nMeilleur k trouvé: {best_k['k']}\")\n",
    "print(f\"Meilleur AUC: {best_k['Average AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Le calcul des métriques au sein du train data-set n'est pas pertinent ici (100% à chaque métrique), évaluons le model KNN en l'entrainant sur l'ensemble des données d'entraintement (80% des données initiales) pour chaque k et le testant sur l'ensemble de test initial (20% des données initiales, non utilisés pour l'entraintement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold evaluation metrics for each k\n",
    "knn_results_by_k = []\n",
    "\n",
    "# Créer une figure pour les courbes ROC\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, 10))  # 8 couleurs différentes pour k=1 à 10\n",
    "\n",
    "# Range of k values to test\n",
    "for idx, k in enumerate(range(1, 11)):\n",
    "    # Initialize the k-NN classifier with the current k value\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    # Perform 5-fold cross-validation for different metrics\n",
    "    accuracy_scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    precision_scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='precision')\n",
    "    recall_scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='recall')\n",
    "    f1_scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='f1')\n",
    "    roc_auc_scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "    \n",
    "    # Get probability predictions for ROC curve\n",
    "    y_pred_proba = cross_val_predict(knn, X_train, y_train, cv=5, method='predict_proba')\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_train, y_pred_proba[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.plot(fpr, tpr, color=colors[idx],\n",
    "             label=f'ROC k={k} (AUC = {roc_auc:.2f})',\n",
    "             lw=2, alpha=0.8)\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_accuracy = np.mean(accuracy_scores)\n",
    "    avg_precision = np.mean(precision_scores)\n",
    "    avg_recall = np.mean(recall_scores)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    avg_auc = np.mean(roc_auc_scores)\n",
    "    \n",
    "    # Calculate standard deviations\n",
    "    std_accuracy = np.std(accuracy_scores)\n",
    "    std_precision = np.std(precision_scores)\n",
    "    std_recall = np.std(recall_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "    std_auc = np.std(roc_auc_scores)\n",
    "    \n",
    "    # Store the results for this k\n",
    "    knn_results_by_k.append({\n",
    "        \"k\": k,\n",
    "        \"Average AUC\": avg_auc,\n",
    "        \"Std AUC\": std_auc,\n",
    "        \"Average Accuracy\": avg_accuracy,\n",
    "        \"Std Accuracy\": std_accuracy,\n",
    "        \"Average Precision\": avg_precision,\n",
    "        \"Std Precision\": std_precision,\n",
    "        \"Average Recall\": avg_recall,\n",
    "        \"Std Recall\": std_recall,\n",
    "        \"Average F1 Score\": avg_f1,\n",
    "        \"Std F1 Score\": std_f1\n",
    "    })\n",
    "\n",
    "# Finaliser le graphique ROC\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', alpha=0.8,\n",
    "         label='Chance (AUC = 0.5)')\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title('Courbes ROC pour différentes valeurs de k')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print out the results for each k\n",
    "for result in knn_results_by_k:\n",
    "    print(f\"\\nResults for k = {result['k']}:\")\n",
    "    print(f\"Average AUC: {result['Average AUC']:.4f} ± {result['Std AUC']:.4f}\")\n",
    "    print(f\"Average Accuracy: {result['Average Accuracy']:.4f} ± {result['Std Accuracy']:.4f}\")\n",
    "    print(f\"Average Precision: {result['Average Precision']:.4f} ± {result['Std Precision']:.4f}\")\n",
    "    print(f\"Average Recall: {result['Average Recall']:.4f} ± {result['Std Recall']:.4f}\")\n",
    "    print(f\"Average F1 Score: {result['Average F1 Score']:.4f} ± {result['Std F1 Score']:.4f}\")\n",
    "\n",
    "# Find best k based on AUC\n",
    "best_k = max(knn_results_by_k, key=lambda x: x['Average AUC'])\n",
    "print(f\"\\nBest k found: {best_k['k']}\")\n",
    "print(f\"Best AUC: {best_k['Average AUC']:.4f} ± {best_k['Std AUC']:.4f}\")\n",
    "\n",
    "# Visualiser l'évolution des métriques en fonction de k\n",
    "metrics = ['Average AUC', 'Average Accuracy', 'Average Precision', 'Average Recall', 'Average F1 Score']\n",
    "plt.figure(figsize=(12, 6))\n",
    "for metric in metrics:\n",
    "    plt.plot([result['k'] for result in knn_results_by_k],\n",
    "             [result[metric] for result in knn_results_by_k],\n",
    "             marker='o',\n",
    "             label=metric)\n",
    "\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Évolution des métriques en fonction de k')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# On force le k sur celui qu'on souhaite en fonction du mix recall/précision\n",
    "best_k = {\"k\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# =============== VALIDATION FINALE ===============\n",
    "print(\"\\n=== VALIDATION FINALE SUR L'ENSEMBLE DE TEST ===\")\n",
    "\n",
    "# Initialiser le modèle k-NN classifier avec le meilleur k\n",
    "best_knn = KNeighborsClassifier(n_neighbors=best_k[\"k\"])\n",
    "\n",
    "# Entraîner le modèle\n",
    "best_knn.fit(X_train, y_train)\n",
    "\n",
    "# Prédire sur l'ensemble de test\n",
    "y_pred = best_knn.predict(X_test)\n",
    "y_pred_proba = best_knn.predict_proba(X_test)\n",
    "\n",
    "# Calculer les métriques d'évaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label=1)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "f1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "# Calculer la courbe ROC et l'AUC pour l'ensemble de test\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_pred_proba[:, 1])\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "# Afficher les résultats de validation\n",
    "print(f\"\\nRésultats sur l'ensemble de test:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {roc_auc_test:.4f}\")\n",
    "\n",
    "# Afficher le rapport de classification détaillé\n",
    "print(\"\\nRapport de classification détaillé:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Créer et afficher la matrice de confusion\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Matrice de Confusion (Ensemble de test)')\n",
    "plt.xlabel('Prédictions')\n",
    "plt.ylabel('Valeurs réelles')\n",
    "plt.show()\n",
    "\n",
    "# Tracer la courbe ROC finale sur l'ensemble de test\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_test, tpr_test, color='darkorange',\n",
    "         label=f'ROC curve (AUC = {roc_auc_test:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title('Courbe ROC sur l\\'ensemble de test')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_subsets_NB = []\n",
    "\n",
    "X_train_NB = X_train.astype(float)  # Convert boolean to float (or int) type\n",
    "X_test_NB = X_test.astype(float)    # Same for X_test\n",
    "\n",
    "X_train_NB = X_train_NB - X_train_NB.min()  # Shift all values to be positive\n",
    "X_test_NB = X_test_NB - X_test_NB.min()  # Shift all values to be positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold evaluation metrics for each alpha\n",
    "nb_results_by_alpha = []\n",
    "\n",
    "# Créer une figure pour les courbes ROC\n",
    "plt.figure(figsize=(10, 8))\n",
    "alpha_values = [1e-50,1e-40, 1e-30, 1e-20, 1e-10, 0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(alpha_values)))\n",
    "\n",
    "# Test different alpha values\n",
    "for idx, alpha in enumerate(alpha_values):\n",
    "    # Initialize the Naive Bayes classifier with the current alpha value\n",
    "    nb = MultinomialNB(alpha=alpha)\n",
    "    \n",
    "    # Perform 5-fold cross-validation for different metrics\n",
    "    accuracy_scores = cross_val_score(nb, X_train_NB, y_train, cv=5, scoring='accuracy')\n",
    "    precision_scores = cross_val_score(nb, X_train_NB, y_train, cv=5, scoring='precision')\n",
    "    recall_scores = cross_val_score(nb, X_train_NB, y_train, cv=5, scoring='recall')\n",
    "    f1_scores = cross_val_score(nb, X_train_NB, y_train, cv=5, scoring='f1')\n",
    "    roc_auc_scores = cross_val_score(nb, X_train_NB, y_train, cv=5, scoring='roc_auc')\n",
    "    \n",
    "    # Get probability predictions for ROC curve\n",
    "    y_pred_proba = cross_val_predict(nb, X_train_NB, y_train, cv=5, method='predict_proba')\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_train, y_pred_proba[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.plot(fpr, tpr, color=colors[idx],\n",
    "             label=f'ROC α={alpha:.0e} (AUC = {roc_auc:.2f})',\n",
    "             lw=2, alpha=0.8)\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_accuracy = np.mean(accuracy_scores)\n",
    "    avg_precision = np.mean(precision_scores)\n",
    "    avg_recall = np.mean(recall_scores)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    avg_auc = np.mean(roc_auc_scores)\n",
    "    \n",
    "    # Calculate standard deviations\n",
    "    std_accuracy = np.std(accuracy_scores)\n",
    "    std_precision = np.std(precision_scores)\n",
    "    std_recall = np.std(recall_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "    std_auc = np.std(roc_auc_scores)\n",
    "    \n",
    "    # Store the results for this alpha\n",
    "    nb_results_by_alpha.append({\n",
    "        \"alpha\": alpha,\n",
    "        \"Average AUC\": avg_auc,\n",
    "        \"Std AUC\": std_auc,\n",
    "        \"Average Accuracy\": avg_accuracy,\n",
    "        \"Std Accuracy\": std_accuracy,\n",
    "        \"Average Precision\": avg_precision,\n",
    "        \"Std Precision\": std_precision,\n",
    "        \"Average Recall\": avg_recall,\n",
    "        \"Std Recall\": std_recall,\n",
    "        \"Average F1 Score\": avg_f1,\n",
    "        \"Std F1 Score\": std_f1\n",
    "    })\n",
    "\n",
    "# Finaliser le graphique ROC\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', alpha=0.8,\n",
    "         label='Chance (AUC = 0.5)')\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title('Courbes ROC pour différentes valeurs de α (Naive Bayes)')\n",
    "plt.legend(loc='lower right', bbox_to_anchor=(1.15, 0))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print out the results for each alpha\n",
    "for result in nb_results_by_alpha:\n",
    "    print(f\"\\nResults for α = {result['alpha']:.0e}:\")\n",
    "    print(f\"Average AUC: {result['Average AUC']:.4f} ± {result['Std AUC']:.4f}\")\n",
    "    print(f\"Average Accuracy: {result['Average Accuracy']:.4f} ± {result['Std Accuracy']:.4f}\")\n",
    "    print(f\"Average Precision: {result['Average Precision']:.4f} ± {result['Std Precision']:.4f}\")\n",
    "    print(f\"Average Recall: {result['Average Recall']:.4f} ± {result['Std Recall']:.4f}\")\n",
    "    print(f\"Average F1 Score: {result['Average F1 Score']:.4f} ± {result['Std F1 Score']:.4f}\")\n",
    "\n",
    "# Find best alpha based on AUC\n",
    "best_alpha = max(nb_results_by_alpha, key=lambda x: x['Average AUC'])\n",
    "print(f\"\\nBest α found: {best_alpha['alpha']:.0e}\")\n",
    "print(f\"Best AUC: {best_alpha['Average AUC']:.4f} ± {best_alpha['Std AUC']:.4f}\")\n",
    "\n",
    "# Visualiser l'évolution des métriques en fonction de alpha\n",
    "metrics = ['Average AUC', 'Average Accuracy', 'Average Precision', 'Average Recall', 'Average F1 Score']\n",
    "plt.figure(figsize=(12, 6))\n",
    "for metric in metrics:\n",
    "    plt.plot([result['alpha'] for result in nb_results_by_alpha],\n",
    "             [result[metric] for result in nb_results_by_alpha],\n",
    "             marker='o',\n",
    "             label=metric)\n",
    "\n",
    "plt.xscale('log')  # Échelle logarithmique pour alpha\n",
    "plt.xlabel('α (échelle log)')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Évolution des métriques en fonction de α')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# =============== VALIDATION FINALE ===============\n",
    "print(\"\\n=== VALIDATION FINALE SUR L'ENSEMBLE DE TEST ===\")\n",
    "\n",
    "# Initialiser le modèle Naive Bayes Multinomial avec le meilleur alpha\n",
    "best_nb = MultinomialNB(alpha=best_alpha['alpha'])\n",
    "\n",
    "# Entraîner le modèle\n",
    "best_nb.fit(X_train_NB, y_train)\n",
    "\n",
    "# Prédire sur l'ensemble de test\n",
    "y_pred = best_nb.predict(X_test_NB)\n",
    "y_pred_proba = best_nb.predict_proba(X_test_NB)\n",
    "\n",
    "# Calculer les métriques d'évaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label=1)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "f1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "# Calculer la courbe ROC et l'AUC pour l'ensemble de test\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_pred_proba[:, 1])\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "# Afficher les résultats de validation\n",
    "print(f\"\\nRésultats sur l'ensemble de test:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {roc_auc_test:.4f}\")\n",
    "\n",
    "# Afficher le rapport de classification détaillé\n",
    "print(\"\\nRapport de classification détaillé:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Créer et afficher la matrice de confusion\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Matrice de Confusion (Ensemble de test)')\n",
    "plt.xlabel('Prédictions')\n",
    "plt.ylabel('Valeurs réelles')\n",
    "plt.show()\n",
    "\n",
    "# Tracer la courbe ROC finale sur l'ensemble de test\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_test, tpr_test, color='darkorange',\n",
    "         label=f'ROC curve (AUC = {roc_auc_test:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title('Courbe ROC sur l\\'ensemble de test')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold evaluation metrics for each n_estimators\n",
    "rf_results = []\n",
    "\n",
    "# Créer une figure pour les courbes ROC\n",
    "plt.figure(figsize=(10, 8))\n",
    "n_estimators_values = [10, 50, 100, 200, 300, 400, 500]\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(n_estimators_values)))\n",
    "\n",
    "# Test different n_estimators values\n",
    "for idx, n_trees in enumerate(n_estimators_values):\n",
    "    # Initialize the Random Forest classifier\n",
    "    rf = RandomForestClassifier(n_estimators=n_trees, \n",
    "                              random_state=42,\n",
    "                              n_jobs=-1)  # Utiliser tous les processeurs\n",
    "    \n",
    "    # Perform 5-fold cross-validation for different metrics\n",
    "    accuracy_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    precision_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='precision')\n",
    "    recall_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='recall')\n",
    "    f1_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='f1')\n",
    "    roc_auc_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "    \n",
    "    # Get probability predictions for ROC curve\n",
    "    y_pred_proba = cross_val_predict(rf, X_train, y_train, cv=5, method='predict_proba')\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_train, y_pred_proba[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.plot(fpr, tpr, color=colors[idx],\n",
    "             label=f'ROC n_trees={n_trees} (AUC = {roc_auc:.2f})',\n",
    "             lw=2, alpha=0.8)\n",
    "    \n",
    "    # Calculate averages and standard deviations\n",
    "    avg_accuracy = np.mean(accuracy_scores)\n",
    "    avg_precision = np.mean(precision_scores)\n",
    "    avg_recall = np.mean(recall_scores)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    avg_auc = np.mean(roc_auc_scores)\n",
    "    \n",
    "    std_accuracy = np.std(accuracy_scores)\n",
    "    std_precision = np.std(precision_scores)\n",
    "    std_recall = np.std(recall_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "    std_auc = np.std(roc_auc_scores)\n",
    "    \n",
    "    # Store the results\n",
    "    rf_results.append({\n",
    "        \"n_estimators\": n_trees,\n",
    "        \"Average AUC\": avg_auc,\n",
    "        \"Std AUC\": std_auc,\n",
    "        \"Average Accuracy\": avg_accuracy,\n",
    "        \"Std Accuracy\": std_accuracy,\n",
    "        \"Average Precision\": avg_precision,\n",
    "        \"Std Precision\": std_precision,\n",
    "        \"Average Recall\": avg_recall,\n",
    "        \"Std Recall\": std_recall,\n",
    "        \"Average F1 Score\": avg_f1,\n",
    "        \"Std F1 Score\": std_f1\n",
    "    })\n",
    "\n",
    "# Finaliser le graphique ROC\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', alpha=0.8,\n",
    "         label='Chance (AUC = 0.5)')\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title('Courbes ROC pour différentes valeurs de n_estimators (Random Forest)')\n",
    "plt.legend(loc='lower right', bbox_to_anchor=(1.15, 0))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print out the results for each n_estimators\n",
    "for result in rf_results:\n",
    "    print(f\"\\nResults for n_estimators = {result['n_estimators']}:\")\n",
    "    print(f\"Average AUC: {result['Average AUC']:.4f} ± {result['Std AUC']:.4f}\")\n",
    "    print(f\"Average Accuracy: {result['Average Accuracy']:.4f} ± {result['Std Accuracy']:.4f}\")\n",
    "    print(f\"Average Precision: {result['Average Precision']:.4f} ± {result['Std Precision']:.4f}\")\n",
    "    print(f\"Average Recall: {result['Average Recall']:.4f} ± {result['Std Recall']:.4f}\")\n",
    "    print(f\"Average F1 Score: {result['Average F1 Score']:.4f} ± {result['Std F1 Score']:.4f}\")\n",
    "\n",
    "# Find best n_estimators based on AUC\n",
    "best_result = max(rf_results, key=lambda x: x['Average AUC'])\n",
    "print(f\"\\nBest n_estimators found: {best_result['n_estimators']}\")\n",
    "print(f\"Best AUC: {best_result['Average AUC']:.4f} ± {best_result['Std AUC']:.4f}\")\n",
    "\n",
    "# Visualiser l'évolution des métriques en fonction de n_estimators\n",
    "metrics = ['Average AUC', 'Average Accuracy', 'Average Precision', 'Average Recall', 'Average F1 Score']\n",
    "plt.figure(figsize=(12, 6))\n",
    "for metric in metrics:\n",
    "    plt.plot([result['n_estimators'] for result in rf_results],\n",
    "             [result[metric] for result in rf_results],\n",
    "             marker='o',\n",
    "             label=metric)\n",
    "\n",
    "plt.xlabel('Nombre d\\'arbres (n_estimators)')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Évolution des métriques en fonction du nombre d\\'arbres')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we force choosing 200, the best recall\n",
    "best_result['n_estimators'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# =============== VALIDATION FINALE ===============\n",
    "print(\"\\n=== VALIDATION FINALE SUR L'ENSEMBLE DE TEST ===\")\n",
    "\n",
    "# Initialiser le modèle Random Forest avec le meilleur n_estimators\n",
    "best_rf = RandomForestClassifier(n_estimators=best_result['n_estimators'],\n",
    "                                random_state=42,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "# Entraîner le modèle\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Prédire sur l'ensemble de test\n",
    "y_pred = best_rf.predict(X_test)\n",
    "y_pred_proba = best_rf.predict_proba(X_test)\n",
    "\n",
    "# Calculer les métriques d'évaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label=1)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "f1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "# Calculer la courbe ROC et l'AUC pour l'ensemble de test\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_pred_proba[:, 1])\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "# Afficher les résultats de validation\n",
    "print(f\"\\nRésultats sur l'ensemble de test:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {roc_auc_test:.4f}\")\n",
    "\n",
    "# Afficher le rapport de classification détaillé\n",
    "print(\"\\nRapport de classification détaillé:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Créer et afficher la matrice de confusion\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Matrice de Confusion (Ensemble de test)')\n",
    "plt.xlabel('Prédictions')\n",
    "plt.ylabel('Valeurs réelles')\n",
    "plt.show()\n",
    "\n",
    "# Tracer la courbe ROC finale sur l'ensemble de test\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_test, tpr_test, color='darkorange',\n",
    "         label=f'ROC curve (AUC = {roc_auc_test:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title('Courbe ROC sur l\\'ensemble de test')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Afficher l'importance des caractéristiques\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': best_rf.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance.head(20))\n",
    "plt.title('Top 20 des caractéristiques les plus importantes')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Caractéristique')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Function to convert IsolationForest predictions to binary format\n",
    "def convert_predictions(y_pred):\n",
    "    # Convert -1 (anomaly) to 1 and 1 (normal) to 0\n",
    "    return np.where(y_pred == -1, 1, 0)\n",
    "\n",
    "# Initialize lists to store results\n",
    "if_results = []\n",
    "\n",
    "# Créer une figure pour les courbes ROC\n",
    "plt.figure(figsize=(10, 8))\n",
    "contamination_values = [0.01, 0.05, 0.1, 0.15, 0.2]\n",
    "n_estimators_values = [100, 200, 300, 400, 500]\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(contamination_values) * len(n_estimators_values)))\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "color_idx = 0\n",
    "# Test different combinations of parameters\n",
    "for n_estimators in n_estimators_values:\n",
    "    for contamination in contamination_values:\n",
    "        print(f\"Testing n_estimators={n_estimators}, contamination={contamination}\")\n",
    "        \n",
    "        # Initialize the Isolation Forest\n",
    "        if_model = IsolationForest(\n",
    "            n_estimators=n_estimators,\n",
    "            contamination=contamination,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Fit and predict on training data\n",
    "        y_pred = convert_predictions(if_model.fit_predict(X_train_scaled))\n",
    "        decision_scores = -if_model.score_samples(X_train_scaled)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_train, y_pred)\n",
    "        precision = precision_score(y_train, y_pred)\n",
    "        recall = recall_score(y_train, y_pred)\n",
    "        f1 = f1_score(y_train, y_pred)\n",
    "        \n",
    "        # Calculate ROC curve and AUC\n",
    "        fpr, tpr, _ = roc_curve(y_train, decision_scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Plot ROC curve\n",
    "        plt.plot(fpr, tpr, color=colors[color_idx],\n",
    "                label=f'ROC n={n_estimators}, c={contamination:.2f} (AUC = {roc_auc:.2f})',\n",
    "                lw=2, alpha=0.8)\n",
    "        color_idx += 1\n",
    "        \n",
    "        # Store results\n",
    "        if_results.append({\n",
    "            \"n_estimators\": n_estimators,\n",
    "            \"contamination\": contamination,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1,\n",
    "            \"AUC\": roc_auc\n",
    "        })\n",
    "\n",
    "# Finaliser le graphique ROC\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', alpha=0.8,\n",
    "         label='Chance (AUC = 0.5)')\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title('Courbes ROC pour différentes configurations (Isolation Forest)')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print results and find best configuration\n",
    "print(\"\\nRésultats pour chaque configuration:\")\n",
    "for result in if_results:\n",
    "    print(f\"\\nn_estimators={result['n_estimators']}, contamination={result['contamination']:.2f}\")\n",
    "    print(f\"Accuracy: {result['Accuracy']:.4f}\")\n",
    "    print(f\"Precision: {result['Precision']:.4f}\")\n",
    "    print(f\"Recall: {result['Recall']:.4f}\")\n",
    "    print(f\"F1-score: {result['F1 Score']:.4f}\")\n",
    "    print(f\"AUC: {result['AUC']:.4f}\")\n",
    "\n",
    "# Find best configuration based on AUC\n",
    "best_result = max(if_results, key=lambda x: x['AUC'])\n",
    "print(f\"\\nMeilleure configuration:\")\n",
    "print(f\"n_estimators: {best_result['n_estimators']}\")\n",
    "print(f\"contamination: {best_result['contamination']:.2f}\")\n",
    "print(f\"AUC: {best_result['AUC']:.4f}\")\n",
    "\n",
    "# Visualiser l'évolution des métriques\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Créer un subplot pour chaque métrique\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC']\n",
    "for idx, metric in enumerate(metrics, 1):\n",
    "    plt.subplot(2, 3, idx)\n",
    "    \n",
    "    # Créer une matrice pour le heatmap\n",
    "    heatmap_data = np.zeros((len(contamination_values), len(n_estimators_values)))\n",
    "    for i, cont in enumerate(contamination_values):\n",
    "        for j, n_est in enumerate(n_estimators_values):\n",
    "            result = next(r for r in if_results \n",
    "                         if r['contamination'] == cont and r['n_estimators'] == n_est)\n",
    "            heatmap_data[i, j] = result[metric]\n",
    "    \n",
    "    # Tracer le heatmap\n",
    "    sns.heatmap(heatmap_data, \n",
    "                xticklabels=n_estimators_values,\n",
    "                yticklabels=contamination_values,\n",
    "                annot=True, \n",
    "                fmt='.3f',\n",
    "                cmap='YlOrRd')\n",
    "    plt.xlabel('n_estimators')\n",
    "    plt.ylabel('contamination')\n",
    "    plt.title(f'{metric} par configuration')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### On constate qu'en jouant avec le paramètre contamination entre 0.05 et 0.01, on peut choisir d'ajuster le recall. On peut détecter toutes les anomalies sans exception mais les faux positifs restent importants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# On force le choix pour avoir un bon recall\n",
    "best_result['n_estimators'] = 300\n",
    "best_result['contamination'] = 0.021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# =============== VALIDATION FINALE ===============\n",
    "print(\"\\n=== VALIDATION FINALE SUR L'ENSEMBLE DE TEST ===\")\n",
    "\n",
    "# Initialiser le modèle Isolation Forest avec la meilleure configuration\n",
    "best_if = IsolationForest(\n",
    "    n_estimators=best_result['n_estimators'],\n",
    "    contamination=best_result['contamination'],\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Entraîner le modèle\n",
    "best_if.fit(X_train_scaled)\n",
    "\n",
    "# Prédire sur l'ensemble de test\n",
    "y_pred_test = convert_predictions(best_if.predict(X_test_scaled))\n",
    "decision_scores_test = -best_if.score_samples(X_test_scaled)\n",
    "\n",
    "# Calculer les métriques d'évaluation\n",
    "accuracy = accuracy_score(y_test, y_pred_test)\n",
    "precision = precision_score(y_test, y_pred_test)\n",
    "recall = recall_score(y_test, y_pred_test)\n",
    "f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "# Calculer la courbe ROC et l'AUC pour l'ensemble de test\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, decision_scores_test)\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "# Afficher les résultats de validation\n",
    "print(f\"\\nRésultats sur l'ensemble de test:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {roc_auc_test:.4f}\")\n",
    "\n",
    "# Afficher le rapport de classification détaillé\n",
    "print(\"\\nRapport de classification détaillé:\")\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "\n",
    "# Créer et afficher la matrice de confusion\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Matrice de Confusion (Ensemble de test)')\n",
    "plt.xlabel('Prédictions')\n",
    "plt.ylabel('Valeurs réelles')\n",
    "plt.show()\n",
    "\n",
    "# Tracer la courbe ROC finale sur l'ensemble de test\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_test, tpr_test, color='darkorange',\n",
    "         label=f'ROC curve (AUC = {roc_auc_test:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title('Courbe ROC sur l\\'ensemble de test')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualisation de la distribution des scores de décision\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(decision_scores_test, bins=50, density=True, alpha=0.7)\n",
    "plt.axvline(np.percentile(decision_scores_test, 100*(1-best_result['contamination'])),\n",
    "            color='r', linestyle='--', label='Seuil de décision')\n",
    "plt.xlabel('Score d\\'anomalie')\n",
    "plt.ylabel('Densité')\n",
    "plt.title('Distribution des scores d\\'anomalie sur l\\'ensemble de test')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
